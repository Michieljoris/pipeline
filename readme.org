* Description
Minimal extendable parallel pipeline for processing data in multiple stages,
variable task/thread count. Each stage can return zero or more results, each
will be processed further. Results are unordered.

A task for processing data should return results asynchronously through a
channel. The function itself should return as soon as possible. When work is
finished a `done` function should be called. A task can be a thread, go block,
async io or any other async function. A `work` function is used to perform
the task.

A `queue?` function is called on all source and subsequent task results to
decide on continuation of processing. This means that the source and result
elements need to carry data to make that decision. Similar to the `work`
function which will also have to inspect the data to decide on how to process
it.

Advantages of a decent abstraction:

- Because decisions on processing are made by inspecting data elements
  themselves pipelines can be dynamic and adapted to each element at any stage.

- Multiple pipelines can be run at the same time, or pipelines can be split
  during processing.

- The `work` function can be instrumented, and these stats can then be used to
  dynamically adjust task count.

- A task can run itself more than one async operation (and claim a task from the
  task pool for each) and for example return all or the first as a (async)
  result.

- Pipelines can be paused by reducing task count to 0. If it's then desirable to
  actually kill off threads they can be taken from a thread pool that allows
  this (jvm let's them linger).

- Errors thrown by transforming functions can be caught and returned as part of
  the wrapped data element.


* An implementation: pipeline.wrapped

Transforming functions are defined in a list of maps, each of which should have
at least one entry under the :xf key, namely the transforming function. This
function takes a single data argument. Returning nil stops the processing of the
data element.

Every data element gets wrapped and carries its own remaining transformations
with it at any stage in the pipeline.

* Utilities:

- Source can be a collection, a channel, a buffered reader (eg. result of (io/reader "file-name")) or a function returning any of these.

- Collect results as set of promises.

- Log every so many elements, and/or periodically.

- Combine transforming functions when possible.

- Ingest csv data


* Rationale
Calculating the minimum number of threads to maximize use of a processor is a
function of the number of cores and the ratio of blocking vs actual work done by
the threads. For example if the threads are waiting (blocking) as much as they
are actually working (keeping a core busy) and we have one core two threads
would keep the core at 1oo% (threads = number of cores * (1 + blocking time /
working time)).

So we don't care which transforming function blocks at all, we just care about
the avarage work/wait ratio over the duration of the job.

As long as there is a one on one relation between source data element and
resulting output element (so no 'multiple' result) xfs can effectively be
combined into one transforming function, and the only rationale for using this
pipeline is the abstracting away of boilerplate such as generalizing source
data, setting up threads, managing life cycle of pipeline (halting), keeping
stats and collecting results.

When transforming functions 'branch' however they can not be combined with
following transforming functions by simple composition, and work will have to be
done in stages.

Regardless, even with branching the ratio of blocking vs waiting will very
likely be stable over the run of the job, and thread count should be set
appropriately. The hooks should help in timing and determining this ratio.

Use core.async's pipeline when results need to be ordered. This pipeline
currentlly makes distinction between blocking and compute work only
conceptually.

* Credits
I started putting together a minimal pipeline with a fixed, then variable number
of threads that wait for tasks to execute, but then took inspiration from
https://stuartsierra.com/2013/12/08/parallel-processing-with-core-async which is
a better abstraction where tasks themselves are parallized, but they don't need
to be threads necessarily. I adapted it to process inputs created from outputs
instead of just processing one transformation in parallel.

Links:
- https://bsless.github.io/mapping-parallel-side-effects/
- https://github.com/tolitius/lasync
- https://github.com/clj-commons/claypoole
- https://github.com/clj-commons/manifold
- https://github.com/clj-commons/dirigiste
- https://github.com/funcool/promesa
- https://github.com/aphyr/tesser
- https://github.com/PureFnOrg/sentenza
- https://github.com/clojure/core.async/blob/master/src/main/clojure/clojure/core/async.clj#L523

* Examples
#+begin_src clojure
;; Minimal pipeline that returns a channel that closes after taking 10 values
(let [{:keys [tasks]} (p/tasks 2) ;; thread count of 2
      xfs [{:xf inc} {:xf inc}] ;; inc each element twice
      source (wrapped/wrap-source (u/channeled (range 10)) xfs)] ;; bind source with xfs
  (p/flow source tasks
          {:queue? wrapped/queue?
           :work   wrapped/thread}))
;;=>
(2 3 4 5 6 7 8 9 10 11) ;;in some unordered order :->
#+end_src

* TODO
