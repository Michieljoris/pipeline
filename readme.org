* Description
Apply a series transforming functions (xfs) to each element in a source channel
in parallel, results are unordered.

Xf functions is a list of maps, each of which should have at least one entry
under the :xf key, namely the transforming function. This function takes a single
data argument. Returning nil stops the processing of the data element.

Every data element gets wrapped and carries its own remaining transformations
with it at any stage in the pipeline. Multiple data sets can be processed
simultaneously each with their own transformations and/or transformations can be
set/replaced during processing. The apply-xf hook can enhance this wrapped data
element at eg. to keep stats, and do things like logging every so many elements.

Threads can be shut down at any time by closing a halt channel.

Intuitively, if we had x number of all-purpose workers/machines/threads and n
items that each require 'work' or transforming in multiple stages we would not
necessarily care about the transformations themselves. The worker would have to
know what to do when accepting an item but the item could tell the worker
itself. Once processed the item might need further processing, if so it will be
added to the workers queue again (albeit a higher priority queue to ensure all
transformations are applied).

By using hooks:

- Errors thrown by xf functions can be caught and returned as part of the wrapped data element (see pipeline.catch-ex).

- Thread count can be varied on the fly, or set to zero to pause job.

- If xf map has an entry :mult set to truthy result is expected to be some sort of list of data elements, and each will be processed separately (see pipeline.mult).

By using util functions:

- Source can be a collection, a channel, a buffered reader (eg. result of (io/reader "file-name")) or a function returning any of these.

- Collect results as set of promises.

- Log every so many elements, and/or periodically.

- Combine transforming functions when possible.

- Ingest csv data

* Rationale
Calculating the minimum number of threads to maximize use of a processor is a
function of the number of cores and the ratio of blocking vs actual work done by
the threads. For example if the threads are waiting (blocking) as much as they
are actually working (keeping a core busy) and we have one core two threads
would keep the core at 1oo% (threads = number of cores * (1 + blocking time /
working time)).

So we don't care which transforming function blocks at all, we just care about
the avarage work/wait ratio over the duration of the job.

As long as there is a one on one relation between source data element and
resulting output element (so no 'multiple' result) xfs can effectively be
combined into one transforming function, and the only rationale for using this
pipeline is the abstracting away of boilerplate such as generalizing source
data, setting up threads, managing life cycle of pipeline (halting), keeping
stats and collecting results.

When transforming functions 'branch' however they can not be combined with
following transforming functions by simple composition, and work will have to be
done in stages.

Regardless, even with branching the ratio of blocking vs waiting will very
likely be stable over the run of the job, and thread count should be set
appropriately. The various hooks should help in timing and determining this
ratio.

Use core.async's pipeline when results need to be ordered. This pipeline
currentlly makes distinction between blocking and compute work only
conceptually.

* Examples
#+begin_src clojure
;; Minimal pipeline that returns a channel that closes after taking 5 values
;; which will be 2,3,4,5,6 in an undetermined order
(p/flow (u/channeled (range 5))
        (p/as-pipe [{:xf inc}
                    {:xf inc}])
        (p/worker 1))
#+end_src

* TODO
