#+title: Readme

* Description
Apply transforming functions (xfs) to a source in parallel/concurrently using a
fixed number of threads. If an xf returns a multiple result each can be
processed further. Results are unordered.

Threads can be shut down at any time by closing a halt channel.

Source can be a collection, a channel, a buffered reader (eg. result of
(io/reader "file-name")) or a function returning any of these.

Xf functions is a list of maps, each of which should have at least one entry
under the :f key, namely the transforming function. This function takes a single
data argument. Returning nil stops the processing of the data element. If xf map
has an entry :mult set to truthy result is expected to be some sort of list of
data elements, and each will be processed separately.

By passing in a pre-xf and/or post-xf each step can be monitored/debugged.

Hooks can be passed for the start and end of processing of the data set and the
first queuing and end of processing of each data element.

Every data element gets wrapped and carries its own remaining transformations
with it at any stage in the pipeline. Because of this the same threads could
process multiple datasets/xfs pairs. Or each element could be processed
differently.

Errors thrown by xf functions are caught and returned as part of the wrapped
data element.

Final result is a set of promises for each return type (result, error and nil).

Intuitively, if we had x number of all-purpose workers/machines/threads and n
items that each require 'work' or transforming in multiple stages we would not
necessarily care about the transformations themselves. The worker would have to
know what to do when accepting an item but the item could tell the worker
itself. Once processed the item might need further processing, if so it will be
added to the workers queue again (albeit a higher priority queue to ensure all
transformations are applied).

Data elements are wrapped so they can carry their (wrapped) transformations with
them. The pre-xf, post-xf and on-queue hooks can also enhance this wrapped data
element at any time, but also use it to keep stats, and do things like logging
every so many elements.

* Rationale
Calculating the minimum number of threads to maximize use of a processor is a
function of the number of cores and the ratio of blocking vs actual work done by
the threads. For example if the threads are waiting (blocking) as much as they
are actually working (keeping a core busy) and we have one core two threads
would keep the core at 1oo% (threads = number of cores * (1 + blocking time /
working time)).

So we don't care which transforming function blocks at all, we just care about
the avarage work/wait ratio over the duration of the job.

As long as there is a one on one relation between source data element and
resulting output element (so no 'multiple' result) xfs can effectively be
combined into one transforming function, and the only rationale for using this
pipeline is the abstracting away of boilerplate such as generalizing source
data, setting up threads, managing life cycle of pipeline (halting), keeping
stats and collecting results.

When transforming functions 'branch' however they can not be combined with
following transforming functions by simple composition, and work will have to be
done in stages.

Regardless, even with branching the ratio of blocking vs waiting will very
likely be stable over the run of the job, and thread count should be set
appropriately. The various hooks should help in timing and determining this
ratio.

Use core.async's pipeline when results need to be ordered. This pipeline
currentlly makes distinction between blocking and compute work only
conceptually.


* Examples
#+begin_src clojure
(let [source (fn [] (a/to-chan! (map #(hash-map :id %) (range 50))))
      xfs [{:f #(assoc % :step-1 true)}
           {:f #(assoc % :step-2 true)}]
      halt (a/chan)
      pipeline (p/pipeline xfs (p/threads 10 (count xfs) halt))
      {:keys [result]} (p/flow source pipeline
                               {:n 3
                                :on-processed (fn [update-collect x status]
                                                 (update-collect))})]
    (map :data @result))
=>  ({:step-2 true
       :step-1 true
       :id 1}
      {:step-2 true
       :step-1 true
       :id 2}
      {:step-2 true
       :step-1 true
       :id 0}
#+end_src

* TODO
